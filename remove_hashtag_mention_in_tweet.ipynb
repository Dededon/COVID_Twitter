{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from natsort import index_natsorted, order_by_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\aruba\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\aruba\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aruba\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\aruba\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\aruba\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement urllib.parse (from versions: none)\n",
      "ERROR: No matching distribution found for urllib.parse\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rt(string):\n",
    "    return re.sub(r\"^RT @\\w \", \"\", string)\n",
    "\n",
    "# From \"@musicmadmarc @SocialDilemma_ @netflix @Facebook I'm also reminded of the very poignant quote by French philosopher… https://t.co/CA52aepW6K\"\n",
    "# to: \"I'm also reminded of the very poignant quote by French philosopher… https://t.co/CA52aepW6K\"\n",
    "\n",
    "def remove_beginning_mentions(string):\n",
    "#     print(string)\n",
    "#     if(re.search(r'@\\w+',string) != None):\n",
    "    # if(string =='@'):\n",
    "    new_string=re.sub(r'^[@\\w ]*','',string,1)\n",
    "    new_string = new_string.strip()\n",
    "    string = remove_beginning_mentions(new_string)\n",
    "    return string\n",
    "\n",
    "# From \"#musicmadmarc #SocialDilemma_ #netflix #Facebook I'm also reminded of the very poignant quote by French philosopher… https://t.co/CA52aepW6K\"\n",
    "# to: \"I'm also reminded of the very poignant quote by French philosopher… https://t.co/CA52aepW6K\"\n",
    "\n",
    "def remove_beginning_hashs(string):\n",
    "    # if(string[0]=='#'):\n",
    "    new_string=re.sub(r'^[#\\w ]*','',string,1)\n",
    "    new_string = new_string.strip()\n",
    "    string = remove_beginning_mentions(new_string)\n",
    "    return string\n",
    "\n",
    "# From \" I'm also reminded of the very poignant quote by French philosopher… https://t.co/CA52aepW6K @musicmadmarc @SocialDilemma_ @netflix @Facebook\"\n",
    "# to: \"I'm also reminded of the very poignant quote by French philosopher… https://t.co/CA52aepW6K\"\n",
    "\n",
    "def remove_last_mentions(string):\n",
    "    last_mention = string.rfind('@')\n",
    "    last_space = string.rfind(' ')\n",
    "    if(last_mention > last_space):\n",
    "        string = string[:last_space]\n",
    "        string = remove_last_mentions(string)\n",
    "    return string\n",
    "\n",
    "# From \" I'm also reminded of the very poignant quote by French philosopher… https://t.co/CA52aepW6K @musicmadmarc @SocialDilemma_ @netflix @Facebook\"\n",
    "# to: \"I'm also reminded of the very poignant quote by French philosopher… https://t.co/CA52aepW6K\"\n",
    "\n",
    "def remove_last_hashs(string):\n",
    "    last_mention = string.rfind('#')\n",
    "    last_space = string.rfind(' ')\n",
    "    if(last_mention > last_space):\n",
    "        string = string[:last_space]\n",
    "        string = remove_last_mentions(string)\n",
    "    return string\n",
    "\n",
    "def remove_punctuation_notation(string):\n",
    "    string = re.sub(r'[\\$%&\\'()*+\\-\\/=#@\\[\\\\\\]^_`{|}~]*','',string)\n",
    "    return string\n",
    "\n",
    "def preprocessing_pipeline(string):\n",
    "    string = remove_rt(string)\n",
    "    string = remove_beginning_mentions(string)\n",
    "    string = remove_beginning_hashs(string)\n",
    "    string = remove_last_mentions(string)\n",
    "    string = remove_last_hashs(string)\n",
    "    string = remove_punctuation_notation(string)\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = pd.read_csv('./output/mask_text.csv')\n",
    "vaccine = pd.read_csv('./data/vaccine_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "# Input the t.co format url and return the original link\n",
    "def to_source_url(shorturl):\n",
    "    try:\n",
    "        r = requests.get(shorturl)\n",
    "        src = urlsplit(r.url)[1]\n",
    "        return src\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_url(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(vaccine['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really love that shirt at\n",
      "Titanic tragedy could have been prevented Economic Times Telegraph co ukTitanic tragedy could have been preve\n",
      "I am at Starbucks 7419 3rd ave at 75th Brooklyn\n"
     ]
    }
   ],
   "source": [
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#','RT @']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "tests = [\n",
    "    \"@peter I really love that shirt at #Macy. http://bet.ly//WjdiW4\",\n",
    "    \"@shawn Titanic tragedy could have been prevented Economic Times: Telegraph.co.ukTitanic tragedy could have been preve... http://bet.ly/tuN2wx\",\n",
    "    \"I am at Starbucks http://4sh.com/samqUI (7419 3rd ave, at 75th, Brooklyn)\",\n",
    "]\n",
    "for t in tests:\n",
    "    print(strip_all_entities(strip_links(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(vaccine['text'])\n",
    "filtered = []\n",
    "urls = []\n",
    "for t in text:\n",
    "    result = find_url(t)\n",
    "    url_list = []\n",
    "    for r in result:\n",
    "        url_list.append(r[0])\n",
    "    urls.append(url_list)    \n",
    "    cleared_text = strip_links(t)\n",
    "    cleared_text = strip_all_entities(cleared_text)\n",
    "    filtered.append(cleared_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['filtered text'] = filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine['urls'] = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine = vaccine.reindex(index=order_by_index(vaccine.index, index_natsorted(vaccine['user'])))\n",
    "vaccine = vaccine.drop_duplicates(subset='                    tweet_id', keep='last')\n",
    "vaccine = vaccine.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(vaccine['user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = vaccine['urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result = pd.read_csv('./results/pure_accumulated_mention_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cluster_0 = set(cluster_result[cluster_result['cluster']==0]['node']) # the general cluster\n",
    "set_cluster_1 = set(cluster_result[cluster_result['cluster']==1]['node']) # Maybe GOP Cluster\n",
    "set_cluster_2 = set(cluster_result[cluster_result['cluster']==2]['node']) # Maybe GOP Cluster\n",
    "# Will Come back to other clusters later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_cluster(index):\n",
    "    return set(cluster_result[cluster_result['cluster']==index]['node'])\n",
    "\n",
    "def fetch_url_statistics(set_cluster):\n",
    "    set_id_clusterzero = set()\n",
    "    for i in range(len(users)):\n",
    "        if users[i] in set_cluster:\n",
    "            set_id_clusterzero.add(i)\n",
    "    urls_zero = list()\n",
    "    for index in set_id_clusterzero:\n",
    "        urls_zero.append(urls[index])\n",
    "    urls_zero_fetched = list()\n",
    "    for lists in urls_zero:\n",
    "        if lists != []:\n",
    "            for item in lists:\n",
    "                urls_zero_fetched.append(item)\n",
    "    return urls_zero_fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1_fetched = fetch_url_statistics(set_cluster_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_src_url(list_url_fecthed):\n",
    "    print(len(list_url_fecthed))\n",
    "    src_urls = []\n",
    "    count = 0\n",
    "    for urls in list_url_fecthed:\n",
    "        src_urls.append(to_source_url(urls))\n",
    "        count += 1\n",
    "        print(count)\n",
    "    return src_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "src_0 = src_urls\n",
    "src_1 = get_src_url(url_1_fetched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_url(src_list):\n",
    "    htfreq = [src_list.count(src) for src in src_list]\n",
    "    htdict = dict(list(zip(src_list, htfreq)))\n",
    "    sorted_ht = sorted(htdict.items(), key = lambda k: k[1], reverse=True)\n",
    "    return sorted_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter.com', 30),\n",
       " (None, 7),\n",
       " ('edition.cnn.com', 4),\n",
       " ('apple.news', 2),\n",
       " ('www.fda.gov', 2),\n",
       " ('joebiden.com', 1),\n",
       " ('www.amazon.com', 1),\n",
       " ('www.haaretz.com', 1),\n",
       " ('nation.africa', 1),\n",
       " ('t.co', 1),\n",
       " ('iwillvote.com', 1),\n",
       " ('www.survivorcorps.com', 1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_url(src_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter.com', 98),\n",
       " ('edition.cnn.com', 20),\n",
       " (None, 13),\n",
       " ('www.reuters.com', 10),\n",
       " ('www.youtube.com', 8),\n",
       " ('www.nytimes.com', 5),\n",
       " ('www.arabnews.com', 3),\n",
       " ('mobile.twitter.com', 3),\n",
       " ('www.msn.com', 3),\n",
       " ('principia-scientific.com', 3),\n",
       " ('www.ons.gov.uk', 2),\n",
       " ('www.fiercebiotech.com', 2),\n",
       " ('en.baaghitv.com', 2),\n",
       " ('covidactnow.org', 2),\n",
       " ('www.independent.co.uk', 1),\n",
       " ('www.bloomberg.com', 1),\n",
       " ('www.instagram.com', 1),\n",
       " ('www.who.int', 1),\n",
       " ('news.sky.com', 1),\n",
       " ('apnews.com', 1),\n",
       " ('time.com', 1),\n",
       " ('www.msnbc.com', 1),\n",
       " ('www.mayoclinic.org', 1),\n",
       " ('www.foreignminister.gov.au', 1),\n",
       " ('m.facebook.com', 1),\n",
       " ('pakistantimestoday.com', 1),\n",
       " ('www.deccanchronicle.com', 1),\n",
       " ('www.kiro7.com', 1),\n",
       " ('investor.agenusbio.com', 1),\n",
       " ('boston.cbslocal.com', 1),\n",
       " ('www.channelnewsasia.com', 1),\n",
       " ('www.washingtonpost.com', 1),\n",
       " ('www.usatoday.com', 1),\n",
       " ('www.ft.com', 1),\n",
       " ('www.nature.com', 1),\n",
       " ('www.theguardian.com', 1),\n",
       " ('www.propublica.org', 1),\n",
       " ('hbr.org', 1),\n",
       " ('www.itamilradar.com', 1),\n",
       " ('coronavirus.house.gov', 1),\n",
       " ('www.retaildive.com', 1),\n",
       " ('www.cell.com', 1),\n",
       " ('www.icosavax.com', 1),\n",
       " ('111.nhs.uk', 1),\n",
       " ('indianexpress.com', 1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_url(src_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['twitter.com',\n",
       " 'www.youtube.com',\n",
       " 'www.telegraph.co.uk',\n",
       " 'insiderfinancial.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " '17plus.weebly.com',\n",
       " 'twitter.com',\n",
       " '17plus.weebly.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'edition.cnn.com',\n",
       " 'www.khaleejtimes.com',\n",
       " 'www.youtube.com',\n",
       " 'twitter.com',\n",
       " 'www.nytimes.com',\n",
       " 'twitter.com',\n",
       " 'www.cnbc.com',\n",
       " 'nypost.com',\n",
       " 'nypost.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'www.msn.com',\n",
       " 'www.nytimes.com',\n",
       " 'www.newsweek.com',\n",
       " 'twitter.com',\n",
       " 'www.foxnews.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'linkinghub.elsevier.com',\n",
       " 'linkinghub.elsevier.com',\n",
       " 'www.belfasttelegraph.co.uk',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'mailchi.mp',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'newswrapped.com',\n",
       " 'newswrapped.com',\n",
       " 'newswrapped.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'www.businessinsider.com',\n",
       " 'www.nytimes.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'www.bloomberg.com',\n",
       " 'twitter.com']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_cluster_2 = acquire_cluster(2)\n",
    "url_2_fetched = fetch_url_statistics(set_cluster_2)\n",
    "src_2 = get_src_url(url_2_fetched)\n",
    "src_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter.com', 34),\n",
       " ('www.nytimes.com', 3),\n",
       " ('newswrapped.com', 3),\n",
       " ('www.youtube.com', 2),\n",
       " ('17plus.weebly.com', 2),\n",
       " ('nypost.com', 2),\n",
       " ('linkinghub.elsevier.com', 2),\n",
       " ('www.telegraph.co.uk', 1),\n",
       " ('insiderfinancial.com', 1),\n",
       " ('edition.cnn.com', 1),\n",
       " ('www.khaleejtimes.com', 1),\n",
       " ('www.cnbc.com', 1),\n",
       " ('www.msn.com', 1),\n",
       " ('www.newsweek.com', 1),\n",
       " ('www.foxnews.com', 1),\n",
       " ('www.belfasttelegraph.co.uk', 1),\n",
       " ('mailchi.mp', 1),\n",
       " ('www.businessinsider.com', 1),\n",
       " ('www.bloomberg.com', 1)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_url(src_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['twitter.com',\n",
       " 'www.youtube.com',\n",
       " 'www.telegraph.co.uk',\n",
       " 'insiderfinancial.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " '17plus.weebly.com',\n",
       " 'twitter.com',\n",
       " '17plus.weebly.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'edition.cnn.com',\n",
       " 'www.khaleejtimes.com',\n",
       " 'www.youtube.com',\n",
       " 'twitter.com',\n",
       " 'www.nytimes.com',\n",
       " 'twitter.com',\n",
       " 'www.cnbc.com',\n",
       " 'nypost.com',\n",
       " 'nypost.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'www.msn.com',\n",
       " 'www.nytimes.com',\n",
       " 'www.newsweek.com',\n",
       " 'twitter.com',\n",
       " 'www.foxnews.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'linkinghub.elsevier.com',\n",
       " 'linkinghub.elsevier.com',\n",
       " 'www.belfasttelegraph.co.uk',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'mailchi.mp',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'newswrapped.com',\n",
       " 'newswrapped.com',\n",
       " 'newswrapped.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'www.businessinsider.com',\n",
       " 'www.nytimes.com',\n",
       " 'twitter.com',\n",
       " 'twitter.com',\n",
       " 'www.bloomberg.com',\n",
       " 'twitter.com']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_cluster_3 = acquire_cluster(2)\n",
    "url_3_fetched = fetch_url_statistics(set_cluster_2)\n",
    "src_3 = get_src_url(url_2_fetched)\n",
    "src_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter.com', 34),\n",
       " ('www.nytimes.com', 3),\n",
       " ('newswrapped.com', 3),\n",
       " ('www.youtube.com', 2),\n",
       " ('17plus.weebly.com', 2),\n",
       " ('nypost.com', 2),\n",
       " ('linkinghub.elsevier.com', 2),\n",
       " ('www.telegraph.co.uk', 1),\n",
       " ('insiderfinancial.com', 1),\n",
       " ('edition.cnn.com', 1),\n",
       " ('www.khaleejtimes.com', 1),\n",
       " ('www.cnbc.com', 1),\n",
       " ('www.msn.com', 1),\n",
       " ('www.newsweek.com', 1),\n",
       " ('www.foxnews.com', 1),\n",
       " ('www.belfasttelegraph.co.uk', 1),\n",
       " ('mailchi.mp', 1),\n",
       " ('www.businessinsider.com', 1),\n",
       " ('www.bloomberg.com', 1)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sorted_url(src_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "f36458b81b9c378f43f2f4e4ca80e8aaf260acc6ac27b27c70b16ecb01255b39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
